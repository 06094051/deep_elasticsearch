# Elasticsearch中文社区-5月问题实战精华篇

# 问题1、新的kafka集群监控系统使用golang开发

monitoringkafkaGolang
开源地址：https://github.com/kppotato/kafka_monitor
 
项目使用：golang开发，数据库:prometheus 图形：grafana

# 问题2、请教elasticsearch出现unassigned shards根本原因？
https://elasticsearch.cn/question/4136

medcl回复：

原因肯定是有很多啊，但是要看具体每一次是什么原因引起的，对照表格排查未免不太高效，怎么办？

es 早已帮你想好对策，使用​ Cluster Allocation Explain
API，

会返回集群为什么不分配分片的详细原因，你对照返回的结果，就可以进行有针对性的解决了。
https://www.elastic.co/guide/en/elasticsearch/reference/6.2/cluster-allocation-explain.html

铭毅天下实战：

```
GET /_cluster/allocation/explain
{
  "index": "test",
  "shard": 0,
  "primary": false,
  "current_state": "unassigned",
  "unassigned_info": {
    "reason": "CLUSTER_RECOVERED",
    "at": "2018-05-04T14:54:40.950Z",
    "last_allocation_status": "no_attempt"
  },
  "can_allocate": "no",
  "allocate_explanation": "cannot allocate because allocation is not permitted to any of the nodes",
  "node_allocation_decisions": [
    {
      "node_id": "ikKuXkFvRc-qFCqG99smGg",
      "node_name": "test",
      "transport_address": "127.0.0.1:9300",
      "node_decision": "no",
      "deciders": [
        {
          "decider": "same_shard",
          "decision": "NO",
          "explanation": "the shard cannot be allocated to the same node on which a copy of the shard already exists [[test][0], node[ikKuXkFvRc-qFCqG99smGg], [P], s[STARTED], a[id=bAWZVbRdQXCDfewvAbN85Q]]"
        }
      ]
    }
  ]
}
```

# 问题3：Elasticsearch写入速度越来越慢

https://elasticsearch.cn/question/4112

用sparkstreaming并行写入es，采用的是bulk接口，20s写入一次，当索引刚建立时速度挺快，耗时大概8到12s，而当大概运行半天后，写入速度下降很多，耗时变为18s以上，有时候甚至超过40s，有大哥哥知道是什么问题吗？

```
if(bulkRequest.numberOfActions()>0)
  {
    val bulkResponse = bulkRequest.get()
    if (bulkResponse.hasFailures) {
      println("failed processing bulk index requests " + bulkResponse.buildFailureMessage)
    }
  }
```

写入变慢的原因可能有

1.随着写入索引数据增多，**单个shard数据量太大(超过50GB)**，导致写入速度变慢

2.随着数据写入，**segment合并变得频繁**，es节点负载变高

# 问题4：ES更新某个字段的值


```
POST supp_dev/discovery/type/_update
{
  "script": "ctx._source.marked = count",
  "params": {
    "count": 0
  }
}
```

# 问题5：es中有自动创建了很多索引，怎么禁止？安装了x-pack？

https://elasticsearch.cn/question/4097

方法1：安装了x-pack，这些watcher开头的索引是跟es里watcher监控插件有关。方法1是设置watcher索引保存时间

方法2：action.auto_create_index: false 关闭了自动创建索引。

方法3：禁用watcher功能就行了。
xpack.watcher.enabled: false

# 问题6：elasticsearch打开和关闭索引详细过程是是什么样子的？
https://elasticsearch.cn/question/4069

**打开**：

当你打开一个索引时，首先会发生的事情是将主碎片分配给特定节点上存在的最新版本。

Lucene索引将被打开（可能需要时间），并且将重播事务日志。 

然后，副本（如果有的话）将被分配并与原分片同步（重复使用具有相同数据的现有节点）。

那么，开放一个指数的成本是多少？ 主要的成本是打分片的索引文件（Lucene），并应用事务日志。

您无法真正改变打开Lucene索引所需的时间（除非索引较少的数据/字段），但是您可以在关闭索引之前将flush发送到索引，因此不需要重放事务日志。


**关闭：**

关闭索引时，除了磁盘大小外，它们不会在群集上占用任何资源。

他们的数据保留在节点上，但是它们不可用于搜索或索引，也没有资源（内存，文件句柄等）。
参考：http://t.cn/Ru3i3Fj

# 问题7:时间戳怎么在logstash里转为date并替换@timestamp

日志里有个字段 "createTime": "1526402356365" ，把这个转成时间替换到@timestamp，怎么写filter啊

实现：
```
date {
    match =>["time_local","dd/MMM/yyyy:HH:mm:ss","ISO8601"]
    target => "@timestamp"
  }
```

time_local 原始日志中的日期时间；后面是你日期格式，可以查询官网

# 问题8:logstash向es里面导入mysql数据，表字段名称自动变成了小写

用logstash向es里面导入mysql数据的时候，表字段按照驼峰式命名，但是数据导进去之后，字段名称全都变成了小写。哪位大神知道这个是什么情况啊，es是5.5.0版本

es中索引名称不支持大写，包含#，以_开头。具体查看MetaDataCreateIndexService类的validateIndexName方法


```
public void validateIndexName(String index, ClusterState state) {
        if (state.routingTable().hasIndex(index)) {
            throw new IndexAlreadyExistsException(new Index(index));
        }
        if (state.metaData().hasIndex(index)) {
            throw new IndexAlreadyExistsException(new Index(index));
        }
        if (!Strings.validFileName(index)) {
            throw new InvalidIndexNameException(new Index(index), index, "must not contain the following characters " + Strings.INVALID_FILENAME_CHARS);
        }
        if (index.contains("#")) {
            throw new InvalidIndexNameException(new Index(index), index, "must not contain '#'");
        }
        if (index.charAt(0) == '_') {
            throw new InvalidIndexNameException(new Index(index), index, "must not start with '_'");
        }
        if (!index.toLowerCase(Locale.ROOT).equals(index)) {
            throw new InvalidIndexNameException(new Index(index), index, "must be lowercase");
        }
        int byteCount = 0;
        try {
            byteCount = index.getBytes("UTF-8").length;
        } catch (UnsupportedEncodingException e) {
            // UTF-8 should always be supported, but rethrow this if it is not for some reason
            throw new ElasticsearchException("Unable to determine length of index name", e);
        }
        if (byteCount > MAX_INDEX_NAME_BYTES) {
            throw new InvalidIndexNameException(new Index(index), index,
                    "index name is too long, (" + byteCount +
                    " > " + MAX_INDEX_NAME_BYTES + ")");
        }
        if (state.metaData().hasAlias(index)) {
            throw new InvalidIndexNameException(new Index(index), index, "already exists as alias");
        }
        if (index.equals(".") || index.equals("..")) {
            throw new InvalidIndexNameException(new Index(index), index, "must not be '.' or '..'");
        }
    }
```
https://elasticsearch.cn/question/4138

感谢各位的热心帮助，找到了问题的原因，我是使用logstash同步mysql数据的，

因为在jdbc.conf里面没有添加 lowercase_column_names => "false"  

这个属性，所以logstash默认把查询结果的列明改为了小写，同步进了es，所以就导致es里面看到的字段名称全是小写。

最后总结：es是支持大写字段名称的，问题出在logstash没用好，需要在同步配置中加上 lowercase_column_names => "false"  。

# 问题9：主节点的数据量，乘父节点数，小于总数据量？
![image](https://elasticsearch.cn/uploads/questions/20180508/3a98a51835a38182102511724b71690c.jpg)
如图：当前索引为一个副本，一个节点。
当前节点为：1031多w，按理说总数据量为2062w数据，但为何总数据量显示为：2677多w，远远大于公式计算出来的数据？

回答：
**你的索引里发生过文档删除，就可能会出现上面的情况**。

比如你往一个索引里插入了6条记录，删除了其中的一条。那么使用/_count，你会得到结果5条。
其实ES删除文档时不是真正删除，而只是会做个标记marker。所以索引里实际还存在6个文档。

**什么时候真正删除？**

ES用的是Luncene 管理索引。Luncene 会在满足一定条件时进行merge操作，其中会把所有标记了delete的文档真正的删除并合并segment。

**什么时候deleted不为0？**

如果删除了文档，但是还没有merge，比如这个索引，主本5条记录，实际18条数据。
![image](https://elasticsearch.cn/uploads/answer/20180509/b5eb6a156462ef0b5a75e41e7f247b4c.png)

执行

```
curl -XGET '192.168.3.100:9200/log_test/_stats?total&pretty'
```

你会发现：
![image](https://elasticsearch.cn/uploads/answer/20180509/8ec337be168fa681272f78d9509e8598.png)

强制做merge，将文档真正删除
POST /log_test/_optimize?only_expunge_deletes=true&wait_for_completion=true

你会发现文档数就变成了
5/15

参考源码

```
public class DocsStats implements Streamable, ToXContent {

    long count = 0;
    long deleted = 0;

    public DocsStats() {

    }

    public DocsStats(long count, long deleted) {
        this.count = count;
        this.deleted = deleted;
    }
    ......
}
```

# 问题10：请问查询人与人之间合作度，这种聚合查询怎么写呢？

请问，我想查询出来，与 张三合作超过5本书的人 ，  
我的数据结构是：
======================
{
"书名"：“abc”
“authors”:[
    {"作者"：“张三”}，
    {"作者"：“李四”}，
    {"作者"：“王五”}
]
}
=====================
{
"书名"：“def”
“authors”:[
    {"作者"：“张三”}，
    {"作者"：“李四”}
]
}


```
{
    "order": 10,
    "template": "fetch_example*",
    "settings": {
      "index": {
        "number_of_shards": "1"
      }
    },
    "mappings": {
      "type": {
        "_all": {
          "enabled": false
        },
        "properties": {
          "书名":{
            "type":"string",
            "index": "not_analyzed"
          },
          "作者":{
            "type":"string",
            "index": "not_analyzed"
          },
          "authors":{  
            "type":"nested",
            "properties":{  
               "作者":{ "type":"string",
            "index": "not_analyzed"}
               
            }
         }
        }
      }
    },
    "aliases": {}
  }

```


```
{
  "size": 0,
  "query": {
    "nested": {
      "path": "authors",
      "query": {
        "term": {
          "authors.作者": {
            "value": "张三"
          }
        }
      }
    }
  },
  "aggs": {
    "1": {
      "nested": {
        "path": "authors"
      },
      "aggs": {
        "2": {
          "terms": {
            "field": "authors.作者",
            "size": 10
          },
          "aggs": {
            "filter": {
              "bucket_selector": {
                "buckets_path": {
                  "the_doc_count": "_count"
                },
                "script": "the_doc_count > 2"
              }
            }
          }
        }
      }
    }
  }
}
```

# 问题11：ES聚合如何实现SQL中的having语句

像SQL里面的HAVING语句来过滤聚合结果（例如根据color聚合avg price，要求avg price要大于4000）
采用post_filter好像实现不了


https://elasticsearch.cn/question/4214


回复：

post_filter 是对原始数据做过滤的，不会对聚合起作用，它存在的意义是先做全局的聚合，然后再过滤详细数据，比如一次性获取所有分类的聚合信息和某个分类下的详细数据。可以看官方的示例。
 
你的需求用 pipeline aggregation 中的 bucket selector 可以实现，看下官方的例子好了

https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-bucket-selector-aggregation.html

https://www.elastic.co/guide/cn/elasticsearch/guide/current/_post_filter.html


```
GET cars/_search
{
  "size": 0,
  "aggs": {
    "colors": {
      "terms": {
        "field": "color"
      },
      "aggs": {
        "avg_price": {
          "avg": {
            "field": "price"
          }
        },
        "bucket_filter": {
          "bucket_selector": {
            "buckets_path": {
              "avgPrice": "avg_price"
            },
            "script": "params.avgPrice>4000"
          }
        }
      }
    }
  }
}
```


# 问题12:用logstash读取kafka里的数据怎么能从头开始
https://elasticsearch.cn/question/4209


```
kafka {
bootstrap_servers => "localhost:9092"
topics => "test"
group_id => "logstash"
auto_offset_reset => "earliest"

}
```

我的配置是这样，我想要重新启动logstash的时候，能重新消费kafka，或者是能接着把没有消费完的数据进行消费，需要每次修改group_id吗

是的，offset commit 只有一次机会，一旦运行过了就记录了，此时你要重新从头消费的话，要么你手动去 zk 把 offset 改了，要么改 group，此时当然是改 group id方便了。

至于重启的时候继续消费，默认就是这样的运行方式，不需要再改 group。

# 问题13：logstash导入mysql上亿级别数据的效率问题
https://elasticsearch.cn/question/2891

请教各位：
  当mysql表中数据存在上亿条时，使用logstash同步表中数据到ES，而且使用jdbc_paging_enabled分页时，其是直接将原sql语句作为子查询，拼接offset和limit来实现。当查询出的结果集较大时存在深度分页瓶颈。
  
  可以看到一次查询花费几千秒。
 
当查询出的结果集较小时，比如几百万时，则只要花费几十秒。

请问各位，有什么好办法提高logstash在数据量较大情况下的查询效率吗，

现在测试表中生成了一亿条左右数据，但是当logstash从mysql中查询数据实在是太慢了，几千秒才能查出一次数据，请问有什么好的方式增加速度吗？

或者有没有大神做过类似场景的测试，请大神指教。

实现方法：
我也遇到你同样问题，可以这么解决：
1.给updated_ts时间字段加上索引。
2.分批处理原则
（1）你的SQL：每批处理100000个
SELECT a.party_id AS id ,a.* FROM PARTY_ALL_1 a WHERE a.updated_ts > '2011-11-17 13:23:58' order by a.updated_ts asc LIMIT 100000;
 
（2）logstash分页的时候每次处理50000个
SELECT * FROM (SELECT a.party_id AS id ,a.* FROM PARTY_ALL_1 a WHERE a.updated_ts > '2011-11-17 13:23:58' order by a.updated_ts asc LIMIT 100000) AS `t1` LIMIT 50000 OFFSET 0;
 
SELECT * FROM (SELECT a.party_id AS id ,a.* FROM PARTY_ALL_1 a WHERE a.updated_ts > '2011-11-17 13:23:58' order by a.updated_ts asc LIMIT 100000) AS `t1` LIMIT 50000 OFFSET 50000;
 
(3)处理两次就写一个最后一条记录的updated_ts时间到指定文件。下一个定时任务启动的时候进行循环处理就行，因为每批处理updated_ts都会改变

# 问题14 ：Grok Debugger官网的在线调试地址：

http://grokdebug.herokuapp.com/
 
Grok Debugger中文站：

http://grok.qiexun.net/
 
自己本地搭建：

http://blog.51cto.com/fengwan/1758845
 
# 问题15 es查询结果数量在两个数之间切换

集群有三个节点，5个主分片1个副本分片。
同一个条件（或者直接match_all）连续两次查询出来hits['total']也就是总数不一样，在两个数之间切换
请问下有人知道是什么原因吗？

回复：

你 es 版本是？你用 preference 参数限制下主副分片，看下是否是主副分片问题。不过按理应该不会这样才对。
另外你 restart 一下集群，看是否会变正常。

6.0版本 用preference看了确实有一个值是主分片 另一个值是副本分片

# 问题16 ES数据不能实时更新
es 数据在被修改之后 再发起查询还是会查到未修改前的数据
es服务版本 5.3.3，
es pom版本2.1.0，jest版本2.3，

我使用的是jest httpclient方式，就是前端每次调一个修改数据的接口之后然后回调查询接口，结果就是会查到老数据（就是查到的那条数据是没有被修改的样子），后来尝试了在回调查询的时候增加了一点延时（500ms），效果提升了一点，可以成功查到修改后数据的样子了，但是不太稳定，时而成功时而失败，被这个问题困惑了好久，求解决！

回复：
1）refresh时间，默认1s

2）默认是1秒可见。如果你的需求一定要写完就可见，那在写的时候增加refresh参数，强制刷新即可。但强烈建议不这么干，因为这样会把整个集群拖垮。

# 问题17 elasticsearch如何查询所有index？
https://elasticsearch.cn/question/4263

GET _cat/indices

详细参考：
https://stackoverflow.com/questions/17426521/list-all-indexes-on-elasticsearch-server


```
curl 'localhost:9200/_cat/indices?v'
```

# 问题18：在6.2版本中提到了一个Bucket Script Aggregation 的功能：
 
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-bucket-selector-aggregation.html
 
用它实现了如下SQL
select company_id, count(1) from xxx_table where company_id between A and B group by company_id having count(1) > 1;
 

```
{
  "size": 0,
  "query": {
    "constant_score": {
      "filter": {
        "range": {
          "company_id": {
            "gte": A,
            "lte": B
          }
        }
      }
    }
  },
  "aggs": {
    "count_by_company_id": {
      "terms": {
        "field": "company_id"
      },
      "aggs": {
        "distinct_id": {
          "cardinality": {
            "field": "company_id"
          }
        },
        "id_bucket_filter": {
          "bucket_selector": {
            "buckets_path": {
              "value": "distinct_id"
            },
            "script": "params.value> 1"
          }
        }
      }
    }
  }
}
```
# 问题19：Filter中term和range不能同时用么？


```
{
  "query": {
    "bool": {
      "filter": [
        {
          "term": {
            "name": "kevin"
          }
        },
        {
          "range": {
            "age": {
              "gte": 20,
              "lte": 30
            }
          }
        }
      ]
    }
  }
}
```

# 问题20：跨集群数据同步方案讨论
https://elasticsearch.cn/question/4294
虽然ES本身就是一个分布式系统，但在生产实践中，往往还是让所有ES节点落在单机房内，各个数据中心之间，采用其他方式来同步数据。那么如何来做数据同步，以及如何确保数据一致性，就成了不得不面对的问题。现提出我知道的方案，供大家讨论，也希望大家伙能分享自己的方案~~~

方案一，双写
在数据写入ES时，通过MQ或者其他方式实现数据双写或者多写，目前很多MQ都有数据持久化功能，可以保障数据不丢；再结合ES各种状态码来处理数据重复问题，即可实现多中心数据的最终一致。

方案二，第三方数据同步
例如使用mysql的主从同步功能，在不同数据中心之间，从本机房的mysql同步数据到ES，依托mysql数据一致性来保障ES数据一致。datax,StreamSet均提供了类似功能。

方案三，基于ES translog同步
读取translog，同步并重放，类似于mysql binlog方式。看起来这种方式最干净利落，但涉及到ES底层代码修改，成本也较高，目前已有的实践：。[/url]
抛砖引玉~~~

关于方案1，有兴趣试试我的这个小工具么，目前支持多集群多写了，自带本地磁盘队列，支持扩展外部 MessageQueue，如 Kafka，RabbitMQ。地址: https://github.com/medcl/elasticsearch-proxy​
 
方案2最复杂，需要从原始数据开始进行处理，需要从 MySQL 到 ES 的文档转换，MySQL 的数据表达方式往往不够直观，使用起来不是很方便。
 
方案3，目前 ES 的 Translog 不能直接读取，修改底层代码产生新的分支，不具实际可行性。
 
还有方案么，有的：
 
方案4，ES 正在实现新的 CCR，cross cluster replication, 基于底层 sequence id 机制，实现的 Changes API，一个集群可被另外一个集群或是本集群“订阅”，从而可以实现数据复制，进行同步，可以是跨数据中心集群也可以本地集群的数据同步。


# 问题21：logstash同步乱码
https://elasticsearch.cn/question/4325

mysql 中的数据 logstash-input-jdbc 导入es mysql列名是中文,导入es后，字段名成了乱码，求解

#处理中文乱码问题

```
codec => plain { charset => "UTF-8"}
```

在input中 你加这个了么

