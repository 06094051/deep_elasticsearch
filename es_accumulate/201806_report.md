# 1、elasticsearch搜索简繁体中文的问题
https://elasticsearch.cn/question/4335
各位好，
    我当前有把公司的exchcnage messagetracking log导入到ELK中，其中邮件主旨中有些是简体中文，有些是繁体中文，比如"中国”或者“中國”，
    这些简繁体中文原封不动地都已经存入到ELK中了，但现在我想要的結果是，如果我搜索關鍵字“中国”或者“中國”时，无法同时搜索出简繁体的内容出来，
    只能搜索简体字，出现简体的结果，搜索繁体字，出现繁体的结果（由于我当前使用的是IK中文分词，繁体支持不理想，会把繁体中文切成单个字）。
我看到medcl大神有推荐个简繁体转换的plugin:elasticsearch-analysis-stconvert,  查看说明，好像只对自己输入的关键字进行转换，
我现在的需求是，无论我输入“中国”或者“中國”关键字搜索时，需要把简繁体的结果都显示出来，大家有什么办法吗，感谢。

medcl回复：
你应该在 mapping 里面使用 stconvert，确保索引的时候就已经转成了简体，查询的时候，也同样使用这个 analyzer 进行统一成简体，不就成了。
只是查询的时候用，索引里面还是繁体当然会匹配不上的

# 2、Elasticsearch连续写入索引？
https://elasticsearch.cn/question/4450
ES版本6.x,传到ElasticSearch的日志太分散了，导致写入索引的速度太慢（机械硬盘I/O速度只有30MB/s, 而磁盘压测时连续写入数据可以达到300MB/s）。
请教，如何设置让es可以较连续的写入索引，或者有什么方法可以提高IO效率？

不是日志太分散，而是同步translog磁盘IO太频繁，可以进行以下优化：idex.translog.sync_interval: 5s
index.translog.durability: async

# 3、es怎么实现2个type关联查询，2个type在不同的index下，求大神指导
https://elasticsearch.cn/question/4445
GET index_a,index_b/type_a,type_b/_search

# 4、ES重索引（reindex）时如何不停止写入服务（业务存在少量物理删除文档操作）？
https://elasticsearch.cn/question/4422
当索引比较大，reindex操作是比较耗时的，如果不停止写入服务，这个过程会有热数据（包括文档的删除）流入老索引中，ES的索引别名在一定程度上实现零停机。
但是当存在物理删除操作时（reindex无法识别已删除的文档），由于业务上实时性要求较高，如何保证重索引过程中在不停止写服务的情况下，
保证新老索引间数据的一致性？由于我们是使用数据库的日志（如binlog、oplog）来同步数据写入到ES，
除了重置日志的消费位点，重放一遍reindex期间的文档操作（但这样还是一定程度上会影响实时性），不知有没有更好的方法？望不吝赐教~

wood大叔：
ES的reindex在索引有实时的update/delete的情况下，即使借助alias，也没有办法实现真正的zero down time。  增加新文档比较好办，
通过alias切换写入到新索引，同时reindex做旧->新索引的数据传输即可。 但是update/delete操作针对的文档如果还未从旧索引传输过来，
直接对新索引操作会导致两个索引数据不一致。
 
我能够想到的（一个未经实际验证）的方案，前提是数据库里的文档有一个类似last_update_time字段记录文档最后更新的时间。
然后数据写入新索引的时候，url里带上下面这样的参数:
version_type=external_gt&version=xxxxxx
其中version_type=external_gt表示写入文档的版本号大于已有的文档版本号，或者文档不存在，写入才会成功， 否则会抛版本冲突的异常。  
另外delete操作都要转换成index操作，index的内容可以是一个空文档。
 
这样实时数据写入新索引和reindex可以同时进行。 实时写入的数据应该具有更高的版本，总是能够成功。 
reindex如果遇到版本冲突，说明该文档被实时部分更新过了，已经过时，可以直接放弃跳过。 
 
该方案的缺陷:
1. 要求数据源里的数据具有版本信息，可能因为各种局限，不太容易更改。
2. delete操作必须转化为写入一个空文档。 delete实际上是一个标记文档，并且本身也有版本信息。但是如果后端发生了segment merge,  
delete可能会被合并以后物理清除。 这样delete和对应的版本信息丢失，之后reindex如果写入了旧版本的文档，仍然会有一致性问题。 
但是空文档会增加索引文件的大小，有额外的消耗。  一个可能的缓解办法是在reindex全部做完以后，再做一次空文档的删除。

# 5、Logstash应该往数据节点还是master节点发数据？
https://elasticsearch.cn/question/4420
集群中有两台专门的master结点，和两台data结点。
小白请问各位大神，Logstash的配置文件Output的hosts应该填数据节点还是master节点，还是都填他会自动识别？

【回复】通常master只负责管理集群，业务数据相关的均不要把master牵涉进来。可以增加coordinate节点或ingres节点来做对外沟通。

# 6、ES 对索引进行操作，比如增加文档等，是在主分片和副分片同时操作吗？还是异步写入的？返回码是在什么时候返回？
https://elasticsearch.cn/question/4389
ES版本5.6
ES 索引操作，设置number_of_replicas：1，操作比如增加文档等，向索引中增加文档，是在主分片和副分片同时操作吗？
还是异步写入的？
返回码是在什么时候返回？
【回复】
在ES5.0以前，通过consistency策略控制：按照ES的主副同步策略，主分片本地索引完成后，会将请求forward到副本分片，默认情况下超过半数副本回复后，主分片才响应client端操作成功。
在5.0以后，这个参数已经被wait_for_active_shards参数(默认值1)取代，即默认情况下只要主分片就可以完成写操作，否则阻塞至超时或者满足条件。
在主完成本地索引后，会forward所有 被称作in-sync copies 的列表里的分片，直到列表所有分片响应成功，才响应client。如果部分失败，则会通知master，将故障分片从in-sync copies 列表里剔除，主分片响应client，与此同时，master会分配一个新的分片来接替故障的分片。
in-sync copies 的内容可以通过API
GET /_cluster/state?filter_path=metadata.indices.your-indexname.in_sync_allocations.*,routing_table.indices.your-indexname.*

*读写文档*详细参考官网：https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-replication.html#_a_few_simple_implications

【讨论】
这个问题我一直有个疑问，我用的es2.x版本，官网的文档也是基于2.x的版本，文档的原文这样写的：
Node 3 executes the request on the primary shard. If it is successful, it forwards the request in parallel to the replica shards on Node 1 and Node 2. Once all of the replica shards report success, Node 3 reports success to the coordinating node, which reports success to the client.
简单来说就是主分片写完，在同步到副本分片上，等待所有的分片写完才返回成功（Once all of the replica shards report success），即数据写入，主副本之间采用的是同步写入。
第一个疑问：
这个操作时对一批写入数据而言，还是一条数据而言，个人理解应该是该次写入请求的写入数据量，那么如果我一次写入数据很大，要等主分片和副本分片写完，再返回响应，这个等待时间是否太久；
第二个疑问：
因为es有个translog，就是为了保证数据写入安全，在hbase中，只要数据写入并在WAL日志（类似translog保证数据写入安全的日志文件）记录，即返回数据成功，并没有像es文档所说的等待所有分片写成功再返回；
第三个疑问：
es通过consistency参数控制数据的一致性，说白了就是在什么样情况下允许写入，在5.x中可以看到
"index.write.wait_for_active_shards"默认值为1
    /**
     * The number of active shard copies to check for before proceeding with a write operation.
     */
    public static final Setting<ActiveShardCount> SETTING_WAIT_FOR_ACTIVE_SHARDS =
        new Setting<>("index.write.wait_for_active_shards",
                      "1",
                      ActiveShardCount::parseString,
                      Setting.Property.Dynamic,
                      Setting.Property.IndexScope);
像yayg2008，所说要写入到in-sync copies 的列表里的分片，直到列表所有分片响应成功，才响应client。这样无法就是改变了能否写入的条件（即主分片处于活跃状态写入就可以），但是数据的主副本一致性还是采用的是同步方式，这样是不是效率仍然没有提高？
一起讨论研究研究
    
【回复】你理解的是对的，index.write.wait_for_active_shards参数是控制写的前提条件。in-sync 关注的是写的过程。5.0开始都是同步写所有分片，官方说这样是为了保证读的一致性。所以副本越多，写入越慢，而且如果有一个shard非常慢，就会拖累其他节点。

【官网】这里顺便把官网的原话贴出来：
The index operation only returns after all active shards within the replication group have indexed the document (sync replication).

# 7、数据库中一对多关系的数据，放到es中如何设计
https://elasticsearch.cn/question/4236
如题。
之前处理过得业务都是 多个表之间有关联的，但是都是1-1的关系，这时候我把每个表的查询条件放到索引里面就可以了，取回id从mysql查。
 
但是现在有1-N的关系了，N表有查询条件，除了使用嵌套 父子这种方式，我能想到的方法就是条件存数组，但是总觉得怪怪的。而且有聚合的需求。
 
大家推荐怎么处理比较好？

目前ES主要有以下4种常用的方法来处理数据实体间的关联关系：

（1）Application-side joins（服务端Join或客户端Join） 
这种方式，索引之间完全独立（利于对数据进行标准化处理，如便于上述两种增量同步的实现），由应用端的多次查询来实现近似关联关系查询。这种方法适用于第一个实体只有少量的文档记录的情况（使用ES的terms查询具有上限，默认1024，具体可在elasticsearch.yml中修改），并且最好它们很少改变。这将允许应用程序对结果进行缓存，并避免经常运行第一次查询。

（2）Data denormalization（数据的非规范化） 
这种方式，通俗点就是通过字段冗余，以一张大宽表来实现粗粒度的index，这样可以充分发挥扁平化的优势。但是这是以牺牲索引性能及灵活度为代价的。使用的前提：冗余的字段应该是很少改变的；比较适合与一对少量关系的处理。当业务数据库并非采用非规范化设计时，这时要将数据同步到作为二级索引库的ES中，就很难使用上述增量同步方案，必须进行定制化开发，基于特定业务进行应用开发来处理join关联和实体拼接。

ps：宽表处理在处理一对多、多对多关系时，会有字段冗余问题，适合“一对少量”且这个“一”更新不频繁的应用场景。宽表化处理，在查询阶段如果只需要“一”这部分时，需要进行结果去重处理（可以使用ES5.x的字段折叠特性，但无法准确获取分页总数，产品设计上需采用上拉加载分页方式）

（3）Nested objects（嵌套文档） 
索引性能和查询性能二者不可兼得，必须进行取舍。嵌套文档将实体关系嵌套组合在单文档内部（类似与json的一对多层级结构），这种方式牺牲索引性能（文档内任一属性变化都需要重新索引该文档）来换取查询性能，可以同时返回关系实体，比较适合于一对少量的关系处理。 
ps: 当使用嵌套文档时，使用通用的查询方式是无法访问到的，必须使用合适的查询方式（nested query、nested filter、nested facet等），很多场景下，使用嵌套文档的复杂度在于索引阶段对关联关系的组织拼装。

（4）Parent/child relationships（父子文档） 
父子文档牺牲了一定的查询性能来换取索引性能，适用于一对多的关系处理。其通过两种type的文档来表示父子实体，父子文档的索引是独立的。父-子文档ID映射存储在 Doc Values 中。当映射完全在内存中时， Doc Values 提供对映射的快速处理能力，另一方面当映射非常大时，可以通过溢出到磁盘提供足够的扩展能力。 在查询parent-child替代方案时，发现了一种filter-terms的语法，要求某一字段里有关联实体的ID列表。基本的原理是在terms的时候，对于多项取值，如果在另外的index或者type里已知主键id的情况下，某一字段有这些值，可以直接嵌套查询。具体可参考官方文档的示例：通过用户里的粉丝关系，微博和用户的关系，来查询某个用户的粉丝发表的微博列表。
ps：父子文档相比嵌套文档较灵活，但只适用于“一对大量”且这个“一”不是海量的应用场景，该方式比较耗内存和CPU，这种方式查询比嵌套方式慢5~10倍，且需要使用特定的has_parent和has_child过滤器查询语法，查询结果不能同时返回父子文档（一次join查询只能返回一种类型的文档）。而受限于父子文档必须在同一分片上，ES父子文档在滚动索引、多索引场景下对父子关系存储和联合查询支持得不好，而且子文档type删除比较麻烦（子文档删除必须提供父文档ID）。
 

如果业务端对查询性能要求很高的话，还是建议使用宽表化处理的方式，这样也可以比较好地应对聚合的需求。在索引阶段需要做join处理，查询阶段可能需要做去重处理，分页方式可能也得权衡考虑下。

# 8、es批量替换数据时，磁盘空间会一直增加？

我把udas_now_1里的数据经过清洗后倒入到另外一个索引中，我在持续的跑这部分数据时，会发现我的目标索引（也就是图里的yanqiang2，源索引是：udas_now_1）磁盘空间会一直增大，我对比了一下这两个索引数据条数是一样的，目标索引会随着我重复的跑数据，进而删除的数据增多（之所以会有删除的数据是因为_id一样，当从源索引迁移到目标索引时，如果_id一样就会替换，也就是先删除后插入，我想大家都明白这个地方）。问题就是，我不理解这个多出来的磁盘空间是谁占用的？
es1.png

【medcl】：删除的数据还在，只是标记为删除，在 merge 操作之前，是继续占磁盘的。
实现参考官网：https://www.elastic.co/guide/en/elasticsearch/reference/6.2/indices-forcemerge.html
https://blog.csdn.net/laoyang360/article/details/80038930

# 9、Elasticsearch took和实测的耗时差异大，qps低
https://elasticsearch.cn/question/4433
通过python requests使用http接口查询数据，发现took返回的耗时基本是1或2ms，但是requests.get的耗时都是10ms以上，且压测的线程越多，requests.get耗时越大，16线程的时候，耗时已经近70ms了，但是took还是1或2ms。
集群情况：2机器，每台机器分别部署一个data node和一个master node，1index，5shard1副本，2367681数据，机器内存189G，processor 40，空闲
请问大家rest是用什么访问的，我这个情况是requests导致的吗，还是我部署的不合理导致的

【回复1】漏考虑了：网络传输的延时。如果你请求的字段顿，信息量大（某字段如：cont代表正文内容），可以考虑source减少返回字段来控制
【回复2】_source改成false，不返回字段试试，如果这样速度快了就把字段的store改成true

# 10、ES的分片为什么会迁移呢？发生自动迁移有几种情况？
背景：当节点的磁盘还是很大的情况下，突然报警说该节点的CPU.idle使用量已经超过99%。查看集群状态还是green，发现有十几个分片正在迁移？初步判定应该是分片迁移导致了这个问题。
所以请教一下大家为什么会有分片开始主动的迁移呢？
集群的索引下入速度也是相当大的。虽然报警后很快就恢复了，但是还是担心这次的问题。请教

【回复1】节点脱离会触发重新分配，观察下是否是一个节点的分片往其他迁移。
【回复2——应用场景】我遇到的分片迁移情况：
1.某个节点下线了，然后该节点的副本分片变为主分片，然后该节点恢复上线后，这个时候就会发生分片迁移，好像在head界面上看分片颜色是紫色的，而集群正常的情况下没有遇到分片迁移的情况。

# 11、【安全】X-PACK免费证书无安全验证功能
https://elasticsearch.cn/question/4438
有什么办法可以给ES节点增加安全验证吗，免费证书无验证功能

【medcl】免费版本确实没有安全功能的，有预算的话可以升级 License，或者试试其他的方案，如 SearchGuard 或者用前面挂 Nginx 来做

# 12、假设存在user字段记录用户账号、time字段记录用户登陆时间，如何确定一个月内，哪些用户曾在凌晨两点至四点登陆过？
https://elasticsearch.cn/question/4442
不知道用户每次登录时间是不是记录下来了 还是time字段只保存了最后的登录时间 
 
如果记录下来了 用户又不是很多 可以这样查
{
  "query": {
    "bool": {
      "filter": [
        {
          "range": {
            "time": {
              "gte": "now-30d/d"
            }
          }
        },
        {
          "script": {
            "script": {
              "source": "doc.time.date.getHourOfDay() <=4 && doc.time.date.getHourOfDay() >=2",
              "lang": "painless"
            }
          }
        }
      ]
    }
  },
  "aggs": {
    "user_ids": {
      "terms": {
        "field": "user_id"
      }
    }
  },
  "size": 0
} 

如果数据量特别大，只能把时间单独存一个字段 然后再查询

# 13、同一电脑部署两个节点集群，第二个开启老是找不到集群就自己变成master
https://elasticsearch.cn/question/4440
在centos系统下，我部署了两个节点的集群，第二开开启老是就自己当作master，于是就出现了两个相同的集群，我又布置了一个节点，这第三个节点也找不到集群就自己成为master，现在就有三个相同集群，附有第一个节点添加的elasticsearch.yml配置图片，其余节点配置差不多，请问这怎么解决？？？？？？搞了好久

【根本原因】找到问题所在了，我曾经开启过第一个节点，并且存过数据，所以在data目录里面有数据保存，后来第二个节点，第三个节点都是从第一个节点复制过来的，所以data目录都有相同的数据存在，在进行集群时发现数据一致就很难形成集群，所以解决办法就是删掉data下面的数据可以了

# 14、200并发压测两台主备ES服务器，CPU占用率在90%以上
https://elasticsearch.cn/question/4327
两台ES6.2.2服务器：（ES设置内存3G）
master:8核CPU、16G内存
slave::8核CPU、16G内存
 
模拟200用户进行并发简单查询测试，发现两台ES服务器的CPU都是非常高，到达了90%以上，只是简单的查询测试，没有进行复杂运算，不至于导致CPU飙升吧，大家帮忙分析分析，下面是压测过程的top相关指标：

【思路】
先通过GET _nodes/{node}/hot_threads 查看线程栈，是哪个线程占用cpu高，如果是elasticsearch[{node}][search][T#10]则是查询导致的。

如果是elasticsearch[{node}][bulk][T#1]则是数据写入导致的。
看你提问是查询引起cpu高，可是截图里排在前面的进程有10106,10107等，机器上是混合部署的吗，压测环境需要排除其他进程对es的影响。

# 15、logstash5.X 时差8小时问题
https://elasticsearch.cn/article/650
在filter中处理
 ruby {   
   code => "event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)"   
 }  
 ruby {  
   code => "event.set('@timestamp',event.get('timestamp'))"  
 }  
 mutate {  
   remove_field => ["timestamp"]  
 } 
 
 这是ruby的语法，
 将@timestamp时间加上8小时，然后复制给timestamp;
第二行是将timestamp的值再赋给@timestamp;
第三行是把timestamp去除掉

# 15、【深度】ES重索引（reindex）时如何不停止写入服务（业务存在少量物理删除文档操作）？
https://elasticsearch.cn/question/4422
当索引比较大，reindex操作是比较耗时的，如果不停止写入服务，这个过程会有热数据（包括文档的删除）流入老索引中，ES的索引别名在一定程度上实现零停机。但是当存在物理删除操作时（reindex无法识别已删除的文档），由于业务上实时性要求较高，如何保证重索引过程中在不停止写服务的情况下，保证新老索引间数据的一致性？由于我们是使用数据库的日志（如binlog、oplog）来同步数据写入到ES，除了重置日志的消费位点，重放一遍reindex期间的文档操作（但这样还是一定程度上会影响实时性），不知有没有更好的方法？望不吝赐教~

【Wood】ES的reindex在索引有实时的update/delete的情况下，即使借助alias，也没有办法实现真正的zero down time。  增加新文档比较好办，通过alias切换写入到新索引，同时reindex做旧->新索引的数据传输即可。 但是update/delete操作针对的文档如果还未从旧索引传输过来，直接对新索引操作会导致两个索引数据不一致。
 
我能够想到的（一个未经实际验证）的方案，前提是数据库里的文档有一个类似last_update_time字段记录文档最后更新的时间，用作写入ES文档的版本号。 然后数据写入新索引的时候，url里带上下面这样的参数:
version_type=external_gt&version=xxxxxx
其中version_type=external_gt表示写入文档的版本号大于已有的文档版本号，或者文档不存在，写入才会成功， 否则会抛版本冲突的异常。   另外delete操作都要转换成index操作，index的内容可以是一个空文档。
 
这样实时数据写入新索引和reindex可以同时进行。 实时写入的数据应该具有更高的版本，总是能够成功。  reindex如果遇到版本冲突，说明该文档被实时部分更新过了，已经过时，可以直接放弃跳过。 
 
该方案的缺陷:
1. 要求数据源里的数据具有版本信息，可能因为各种局限，不太容易更改。
2. delete操作必须转化为写入一个空文档。 delete实际上是一个标记文档，并且本身也有版本信息。但是如果后端发生了segment merge,  delete可能会被合并以后物理清除。 这样delete和对应的版本信息丢失，之后reindex如果写入了旧版本的文档，仍然会有一致性问题。  但是空文档会增加索引文件的大小，有额外的消耗。  一个可能的缓解办法是在reindex全部做完以后，再做一次空文档的删除。

# 16、es2.3.4升级es5.5.3出现的问题——关于升级以后慢查询的问题
关于升级以后慢查询的问题，参考 https://elasticsearch.cn/article/446  看一下是否有对数值型字段做term/terms query的。 

【回复】谢谢wood叔，之前看到过您的这篇文章，我们在使用过程中确实type设置的都是数值类型，不过通过profile:true分析不是这块的影响，后面我们通过删除某个节点data目录下的所有文件，重启该节点后，慢查询问题就没有了，

所有应用场景在term/terms的type都改成您建议的keyword。看了这篇文章，很有收获，满满的干货，谢谢！

# 17、请问如何基于query 后的score来搜索
请问如何基于query 后的score来搜索？
 https://elasticsearch.cn/question/4452
我的意思是， 我同时搜索一个index，或者多个index。然后回来的query，比如我只想要socre,10分以上的数据，这种要怎么写query呢？

{
  "query": {
    "match": {"title": "abc"}
  },
  "min_score": 10
}

# 18、上千个es节点是节点发现是怎么配置呢？用的组播方式
https://elasticsearch.cn/question/3396
elasticsearch集群配置
【medcl】组播没有了，使用 unicast，每个节点部署的时候，配置文件里面的 unicast 配置指向固定的 master ip（一般至少2个） 就行了。

# 19、关于SSD的使用场景
https://elasticsearch.cn/question/4468
大家好请教一个问题，写入量很小（每秒大概200-500条doc左右，每日500-1000万doc、10G左右）、聚合业务很多（集群索引25T左右，最大14T（主副本）），有没有必要使用SSD，优势有哪些..（每日写入量很小的情况下）

【回复】关键是查询，你查询能够等待的时间是多少？单个查询能够包括多少数据量，几个副本？
多少节点，这些都有影响的。
查询的话，多节点，多副本能够提供命中率的。

1、查询的要求不高，对查询大部分的场景是使用 scroll 导出指定数据后作分析，根据 keyword 字段 + match_phrase 获取结果。基本没有要求毫秒级别的查询 ，实时返回查询结果的场景只有 kibana.. 
2、所有的单个查询都会有时间范围限定，每个索引都只有1个副本（最大的索引 主分片7.8T ） 、25个data node ，es 5.4.0
3、业务上对es的时候场景只有两种 scroll数据导出 + 大量各种聚合计算（目前使用没有问题），对查询要求和索引速度要求都比较低 

【wood】这种场景没有必要使用SSD。

# 20、es聚合统计数据不准确问题，如何解决？
前端执行sql：select count(distinct external_id) from HUMAN_INFO where facelib_id=2 ，底层走的es查询语句如下：
{
  "query": {
    "bool": {
      "must": {
        "term": {
          "facelib_id": 2
        }
      }
    }
  },
  "size": 0,
  "aggs": {
    "distinct": {
      "cardinality": {
        "field": "external_id"
      }
    }
  }
}

【回复】这里，我又重新阅读官方文档，发现es的cardinality，有一个准确值的问题：precision_threshold，这里贴出官网的原话：
The precision_threshold options allows to trade memory for accuracy, and defines a unique count below which counts are expected to be close to accurate. Above this value, counts might become a bit more fuzzy. The maximum supported value is 40000, thresholds above this number will have the same effect as a threshold of 40000. The default value is 3000.
即这个api是存在一定误差的，后续将查询语句添加precision_threshold参数，还是没能解决，估计是数据太大了，后续研究研究是否有好的解决办法：
{
  "query": {
    "bool": {
      "must": {
        "term": {
          "facelib_id": 2
        }
      }
    }
  },
  "size": 0,
  "aggs": {
    "distinct": {
      "cardinality": {
        "field": "external_id",
        "precision_threshold": 40000
      }
    }
  }
}

官网：https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html

# 21、高并发(20000+QPS)性能问题排查
https://elasticsearch.cn/question/4463
16核/32G服务器10台，支持20000左右的QPS请求，其中某一类请求，大部分99.9%的响应时间都在20ms以内，但每隔1分钟，就会出现连续一串响应时间大于300ms以上的请求，非常规律，优化了索引大小，相关字段norms和doc_values也做了优化，但这个现象还是一直存在。。es的线程队列基本无阻塞、jvm heap也很充足，refresh_interval设置为10s，请教各位大神，可能是什么原因？

【wood】如果GC没有可疑点，线程池没有排队的，那么有可能和请求本身的特性有关系。
 
这个“某一类”请求，是什么样子的，能否给个DSL范例？  如果是非常有规律的每分钟出现一次，有些怀疑是按照时间范围做的range查询，并且精度是分钟级。  因为range filter会被cache起来，在结果集很大的情况下，构造cache耗时会比较长，后面快的查询则可能是因为直接走了cache。 由于精度是分钟级，所以每分钟cache过期，重新构建，规律性出现长耗时查询。
 
在看到DSL进一步分析以前，仅仅是猜测。 

【medcl】服务端和客户端的监控都看看，客户端还可以 profile 一下，猜也不是办法
GC 监控看看

# 22、logstash+elasticsearch，时区问题
https://elasticsearch.cn/question/2729
最近在使用Logstash采集日志到ES中，
发现：
logstash产生的@timestamp字段总是零时区的时间，可是日志中的时间字段却是东八区的时间，然后，我就按照下图所示：

捕获.PNG

我用日志中的时间替换替换@timestamp字段，发现替换后的时间仍然是零时区的时间，然后我就又加了8个小时，发现@timestamp字段可以和日志中的时间一样了。
但是，我在按下图每天在ES中建立一个索引时

捕获123.PNG

发现，ES还是按照零时区的零点来建立索引的，而不是东八区的零点建立新的索引。这就导致，2017-11-01索引下实际存放的是2017-11-01 08:00到2017-11-02 08:00的日志数据。从而导致数据不准确。
现在的疑惑是，ES建立索引的时间是ES的自身问题还是logstash的设置问题？如何控制ES建立索引的时区问题？
想请问，有哪位前辈有对这方面有深入研究，能够解决在下的问题，不胜感激！

实现参考：https://blog.csdn.net/wuyinggui10000/article/details/77879016

# 23、【探讨】es个别节点出现Out of Memory现象，是否由于段合并引起
es日志打印OOM现象，根据日志来看，是否因为写入数据过程中，发生段合并所导致，日志内容如下：

https://elasticsearch.cn/question/4511

# 24、elasticsearch更新,排序
问题:
目前有个搜索列表,数据源直接从es读取,
当我对搜索结果某个值做更新后,再次搜索,这条数据位置就变了,如何能避免这种情况呢
 
经过测试貌似是评分相同的数据,更新后会有排序变动问题;

【wood】评分相同的数据，是按照数据写入的索引的位置来返回的。如果对一条数据做了更新，相当于新插入了一条新数据，数据返回的顺序是有可能出现变化。   由于你是按照评分排序，那么评分相同的数据返回的顺序本来就不可控，我感觉是用例设计上的问题。

# 25、【总结】关于同义词检索方案的一点实践经验
https://elasticsearch.cn/article/671

# 26、elasticsearch 分词后，还可以保留大写吗
https://elasticsearch.cn/question/4558
elasticsearch 分词后，还可以保留大写吗

可以，自定义分词即可，不要用 lowercase 即可
参考：https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-custom-analyzer.html
已解决，我用的是自定义 type:pattern 默认设置了小写， 需要设置 "lowercase": false

# 27、scroll 并发个数有限制吗？
通过scroll进行结果集遍历查询，原理上说是es通过快照实现的，快照肯定是需要耗费一定内存的；
那么对于同时开启很多个scroll进行查询，会不会有问题？有没有个数限制？

【wood】据我所知，ES内部没有限制scroll的并发量，但是过高的scroll并发可能会导致GC频率上升，耗时增加。 主要原因是scroll需要在内存里维护一个search context， 并发量很高的情况下，会有很多对象无法被GC回收，极端情况可能造成长时间的FULL GC，影响集群稳定性。

# 28、搜索结果自定义排序
怎么做到自定义排序，举例如下：
搜索关键字 空调，我们要配置 xxx品牌的，要显示在最前面。
xxx品牌和空调可以是动态调整的。不知道大家有没有做过的经验或者思路，感谢。

加一个should条件
{
  "query": {
    "bool": {
      "must": [
        {"match": {"goods": "空调"}}
      ],
      "should": [
        {"term": {"brand": "xxx"}}
      ]
    }
  }
}

# 29、【精彩】业务数据量与ES集群规模及配置的对应关系大致是怎样？
https://elasticsearch.cn/question/4519
耗时排查工具推荐——就是kibana里dev tool工具项中有个search profiler，将查询贴进去执行就可以了。

# 30、
